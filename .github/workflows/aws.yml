name: Build and upload to S3

on: push

env:
  PYTHON_VERSION: '3.12'
  PROJECT_NAME: ${{ github.event.repository.name }}
  BUILD_PATH: '~/builds'
  AWS_BUCKET_NAME: ${{ secrets.AWS_BUCKET_NAME }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_REGION: ${{ secrets.AWS_REGION }}
  S3_FOLDER_TO_KEEP : 'tagged'
  S3_BUILDS_COUNT_TO_KEEP: 2
  
jobs:
  run-and-upload:
    name: Build and upload to S3
    
    runs-on: self-hosted

    strategy:
      matrix:
        mode: [DEBUG, RELEASE]
        
    env:
      MODE: ${{ matrix.mode }}
        
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set build repo
        run: echo "BUILD_REPO=${{ env.BUILD_PATH }}/${{ env.PROJECT_NAME }}/${{ env.MODE }}" >> $GITHUB_ENV

      - name: Set upload repo
        # When tag pushed, files will be uploaded in the ${{ env.S3_FOLDER_TO_KEEP }} folder
        run: |
          if [[ "${{ github.ref }}" == refs/tags/* ]]; then
            echo "UPLOAD_REPO=${{ env.PROJECT_NAME }}/${{ env.S3_FOLDER_TO_KEEP }}/${{ github.ref_name }}-${{ github.run_number }}/${{ env.MODE }}" >> $GITHUB_ENV
          else
            echo "UPLOAD_REPO=${{ env.PROJECT_NAME }}/${{ github.run_number }}/${{ env.MODE }}" >> $GITHUB_ENV
          fi
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Build ${{ env.PROJECT_NAME }} ${{ env.MODE }}
        run: |
          python build.py ${{ env.BUILD_REPO }} ${{ env.MODE }}
          
      - name: Upload to S3
        #Upload files only if push has tags or the push is made not to the feature/ branches
        if: success() && startsWith(github.ref, 'refs/tags/') || !startsWith(github.ref_name, 'feature/')
        run: |
          #Archiving files
          (cd ${{ env.BUILD_REPO }} && zip -r ${{ env.BUILD_REPO }}.zip .)
          
          #Upload to S3
          aws s3 cp ${{ env.BUILD_REPO }}.zip s3://${{ env.AWS_BUCKET_NAME }}/${{ env.UPLOAD_REPO }}.zip --region ${{ env.AWS_REGION }}
      
  clean-up:
    name: Clean up
    
    needs: run-and-upload
    
    runs-on: self-hosted
    
    steps:
      - name: Clean up working folder
        run: |
          rm -r -f ${{ env.BUILD_PATH }}
          
      - name: Clean old builds from S3
        run: |
          # Retrieve a list of folders in the specified bucket and project prefix.
          folder_list=$(aws s3api list-objects-v2 \
                          --bucket ${{ env.AWS_BUCKET_NAME }} \
                          --prefix ${{ env.PROJECT_NAME }}/ \
                          --delimiter '/' \
                          --query "CommonPrefixes[?Prefix != \`${{ env.PROJECT_NAME }}/${{ env.S3_FOLDER_TO_KEEP }}/\`].Prefix" \
                          --output text)

          # Iterate through each folder to find the latest date and time of its contents.
          folders_with_dates=$(for folder in $folder_list; do
                                echo "$(aws s3 ls s3://${{ env.AWS_BUCKET_NAME }}/$folder --recursive | sort | tail -n 1) $folder"
                              done)

          # Sort folders based on the latest date and time.
          sorted_folders=$(echo "$folders_with_dates" | sort | awk '{print $NF}')
          folder_array=($sorted_folders)

          # Determine the number of folders to delete based on the configured count to keep.
          folders_to_delete=("${folder_array[@]:0:${#folder_array[@]}-${{ env.S3_BUILDS_COUNT_TO_KEEP }}}")

          # Delete the identified folders and their contents from the S3 bucket.
          for folder in "${folders_to_delete[@]}"; do
            aws s3 rm "s3://${{ env.AWS_BUCKET_NAME }}/$folder" --recursive
          done
