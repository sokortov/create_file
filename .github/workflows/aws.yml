name: Build and upload to S3

on: push
      
jobs:
  run-and-upload:
    name: Build and upload to S3
    
    runs-on: self-hosted

    strategy:
      matrix:
        mode: [DEBUG, RELEASE]
        
    env:
      PYTHON_VERSION: '3.12'
      PROJECT_NAME: ${{ github.event.repository.name }}
      MODE: ${{ matrix.mode }}
      BUILD_REPO: ${{ matrix.mode }}
      AWS_BUCKET_NAME: ${{ secrets.AWS_BUCKET_NAME }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
      S3_FOLDER_TO_KEEP : 'tagged'
      S3_BUILDS_COUNT_TO_KEEP: 2
        
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Build ${{ env.PROJECT_NAME }} ${{ env.MODE }}
        run: |
          python build.py ${{ env.BUILD_REPO }} ${{ env.MODE }}
          
      - name: Set upload repo 
        if: success()
        run: |
          #Set upload repo
          if [ -n "${{ github.event.release.tag_name }}" ]; then
            echo "UPLOAD_REPO=${{ github.event.repository.name }}/${{ env.S3_FOLDER_TO_KEEP }}/${{ github.event.release.tag_name }}${{ github.run_number }}/${{ matrix.mode }}" >> $GITHUB_ENV
          else
            echo "UPLOAD_REPO=${{ github.event.repository.name }}/${{ github.run_number }}/${{ matrix.mode }}" >> $GITHUB_ENV
          fi
          
      - name: Upload to S3
        if: success()
        run: |
          #Archiving files
          (cd ${{ env.BUILD_REPO }} && zip -r "$OLDPWD/${{ env.BUILD_REPO }}.zip" .)
          
          #Upload to S3
          aws s3 cp ${{ env.BUILD_REPO }}.zip s3://${{ env.AWS_BUCKET_NAME }}/${{ env.UPLOAD_REPO }}.zip --region ${{ env.AWS_REGION }}

      - name: Clean old builds from S3
        if: success()
        run: |
          # Retrieve a list of folders in the specified bucket and project prefix.
          folder_list=$(aws s3api list-objects-v2 \
                          --bucket ${{ env.AWS_BUCKET_NAME }} \
                          --prefix ${{ env.PROJECT_NAME }}/ \
                          --delimiter '/' \
                          --query "CommonPrefixes[?Prefix != \`${{ env.PROJECT_NAME }}/${{ env.S3_FOLDER_TO_KEEP }}/\`].Prefix" \
                          --output text)
         
          # Iterate through each folder to find the latest date and time of its contents.
          folders_with_dates=$(for folder in $folder_list; do
                                echo "$(aws s3 ls s3://${{ env.AWS_BUCKET_NAME }}/$folder --recursive | sort | tail -n 1) $folder"
                              done)
          
          # Sort folders based on the latest date and time.
          sorted_folders=$(echo "$folders_with_dates" | sort | awk '{print $NF}')
          folder_array=($sorted_folders)
          
          # Determine the number of folders to delete based on the configured count to keep.
          folders_to_delete=("${folder_array[@]:0:${#folder_array[@]}-${{ env.S3_BUILDS_COUNT_TO_KEEP }}}")
          
          # Delete the identified folders and their contents from the S3 bucket.
          for folder in "${folders_to_delete[@]}"; do
            aws s3 rm "s3://${{ env.AWS_BUCKET_NAME }}/$folder" --recursive
          done
          
      - name: Clean up working folder
        run: |
          rm -r -f ${{ env.BUILD_REPO }}
          rm -f ${{ env.BUILD_REPO }}.zip
