name: Build and upload to S3

on: push
      
jobs:
  run-and-upload:
    name: Build and upload to S3
    
    runs-on: self-hosted

    strategy:
      matrix:
        mode: [DEBUG, RELEASE]
        
    env:
      PYTHON_VERSION: '3.12'
      PROJECT_NAME: ${{ github.event.repository.name }}
      MODE: ${{ matrix.mode }}
      BUILD_REPO_NAME: ${{ github.event.repository.name }}/${{ github.sha }}/${{ matrix.mode }}
      AWS_BUCKET_NAME: ${{ secrets.AWS_BUCKET_NAME }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
      PREFIX: ''
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Build ${{ env.PROJECT_NAME }} ${{ env.MODE }}
        run: |
          python build.py ${{ env.BUILD_REPO_NAME }} ${{ env.MODE }}
          
      - name: Get commit message
        id: get_commit
        run: echo "COMMIT_MESSAGE=$(git log -1 --pretty=%B)" >> $GITHUB_ENV
      
      - name: Upload to S3
        if: success()
        run: |
          zip -r ${{ env.BUILD_REPO_NAME }}.zip -j ${{ env.BUILD_REPO_NAME }}
          aws s3 cp ${{ env.BUILD_REPO_NAME }}.zip s3://${{ env.AWS_BUCKET_NAME }}/${{ env.COMMIT_MESSAGE }}/${{ env.BUILD_REPO_NAME }}.zip --region ${{ env.AWS_REGION }}

      - name: List and delete old folders in S3 bucket
        env:
          BUCKET: your-bucket-name
        run: |
          # Получаем список всех объектов в бакете с датами последнего изменения
          OBJECTS=$(aws s3api list-objects-v2 --bucket ${{ env.AWS_BUCKET_NAME }} --query "Contents[].{Key: Key, LastModified: LastModified}" --output json)
        
          # Извлекаем список папок из объектов, содержащих "/"
          FOLDERS=$(echo "$OBJECTS" | jq -r '.[].Key | select(contains("/")) | split("/")[0] | unique')

          # Получаем список папок с датами последнего изменения
          FOLDERS_WITH_DATES=$(for FOLDER in $FOLDERS; do
                                  LAST_MODIFIED=$(aws s3api list-objects-v2 --bucket ${{ env.AWS_BUCKET_NAME }} --prefix "${FOLDER}/" --query "Contents[?ends_with(Key, '/')] | [0].LastModified" --output text)
                                  echo "$FOLDER $LAST_MODIFIED"
                              done)

          # Сортируем папки по дате последнего изменения (самые старые первые)
          SORTED_FOLDERS=$(echo "$FOLDERS_WITH_DATES" | sort -k2)
 
          # Отладочный вывод для проверки содержимого
          echo "Список папок с датами последнего изменения:"
          echo "$SORTED_FOLDERS"

          # Подсчитываем количество уникальных папок
          TOTAL_FOLDERS=$(echo "$SORTED_FOLDERS" | wc -l)

          # Проверка количества папок
          echo "Общее количество папок: $TOTAL_FOLDERS"
  
          # Сколько папок нужно удалить
          DELETE_COUNT=$(($TOTAL_FOLDERS - 5))

          if [ $DELETE_COUNT -gt 0 ]; then
            # Получаем папки, которые нужно удалить
            FOLDERS_TO_DELETE=$(echo "$SORTED_FOLDERS" | head -n $DELETE_COUNT | awk '{print $1}')
          
            # Отладочный вывод для проверки папок на удаление
            echo "Папки для удаления:"
            echo "$FOLDERS_TO_DELETE"
 
            # Удаляем старые папки
            for FOLDER in $FOLDERS_TO_DELETE; do
              aws s3 rm s3://${{ env.AWS_BUCKET_NAME }}/$FOLDER --recursive
            done
          else
            echo "Нет папок для удаления."
          fi
          
      - name: Cleaning up
        run: |
          rm -r -f ${{ env.BUILD_REPO_NAME }}
